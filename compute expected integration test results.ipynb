{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to compute the expected image slices against which the integration tests results will be checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! apt-get update -qq\n",
    "! apt-get install -y -qq libgl1-mesa-glx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/.local/share/virtualenvs/notebooks-TPEo7knB/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from parameterized import parameterized\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from diffusers import Prompt2PromptPipeline, DDIMScheduler, UNet2DModel\n",
    "from diffusers.utils.testing_utils import enable_full_determinism, require_torch_gpu, slow, torch_device\n",
    "\n",
    "np.set_printoptions(linewidth=140, precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_full_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_steps = {\n",
    "    \"cross_replace_steps\": 0.4,\n",
    "    \"self_replace_steps\": 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix = [\n",
    "    (\n",
    "        [\"A turtle playing with a ball\", \"A monkey playing with a ball\"],\n",
    "        \"replace\",\n",
    "        {**replace_steps},\n",
    "        [0.243, 0.233, 0.227, 0.253, 0.242, 0.237, 0.293, 0.292, 0.283]\n",
    "    ), \n",
    "    (\n",
    "        [\"A turtle playing with a ball\", \"A monkey playing with a ball\"],\n",
    "        \"replace\",\n",
    "        {**replace_steps, \"local_blend_words\": [\"turtle\", \"monkey\"]},\n",
    "        [0.243, 0.233, 0.227, 0.253, 0.242, 0.237, 0.293, 0.292, 0.283]\n",
    "    ), \n",
    "    (\n",
    "        [\"A turtle\", \"A turtle in a forest\"],\n",
    "        \"refine\",\n",
    "        {**replace_steps},\n",
    "        [0.256, 0.232, 0.209, 0.259, 0.254, 0.229, 0.285, 0.307, 0.295]\n",
    "    ),\n",
    "    (\n",
    "        [\"A turtle\", \"A turtle in a forest\"],\n",
    "        \"refine\",\n",
    "        {**replace_steps, \"local_blend_words\": [\"in\", \"a\" , \"forest\"]},\n",
    "        [0.256, 0.232, 0.209, 0.259, 0.254, 0.229, 0.285, 0.307, 0.295]\n",
    "    ), \n",
    "    (\n",
    "        [\"A smiling turtle\"] * 2,\n",
    "        \"reweight\",\n",
    "        {**replace_steps, \"equalizer_words\": [\"smiling\"], \"equalizer_strengths\": [5]},\n",
    "        [0.006, 0.010, 0.009, 0.003, 0.011, 0.008, 0.014, 0.009, 0.000]\n",
    "    ), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def expand(test, matrix): return [partial(test, *params) for params in matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  5.22it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "pipe = Prompt2PromptPipeline.from_pretrained(model_id)\n",
    "pipe.to(torch_device)\n",
    "pipe.set_progress_bar_config(disable=None)\n",
    "\n",
    "def test_inference(prompts, edit_type, edit_kwargs, expected_slice, seed=0, n_steps=50):\n",
    "    print(f\"Starting next test ðŸŽ¢ (prompts={prompts}, edit_type={edit_type}, edit_kwargs={edit_kwargs})\")\n",
    "\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    image = pipe(prompts, height=512, width=512, num_inference_steps=n_steps, generator=generator, edit_type=edit_type, edit_kwargs=edit_kwargs, output_type=\"numpy\").images\n",
    "\n",
    "    image_slice = image[0, -3:, -3:, -1]\n",
    "\n",
    "    assert image.shape == (2, 512, 512, 3)\n",
    "    return image_slice.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_tests = expand(test_inference, test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting next test ðŸŽ¢ (prompts=['A turtle playing with a ball', 'A monkey playing with a ball'], edit_type=replace, edit_kwargs={'cross_replace_steps': 0.4, 'self_replace_steps': 0.4})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diffusers/diffusers/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:429: FutureWarning: The decode_latents method is deprecated and will be removed in a future version. Please use VaeImageProcessor instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.243, 0.233, 0.227, 0.253, 0.242, 0.237, 0.293, 0.292, 0.283], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_tests[0]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run it for all test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting next test ðŸŽ¢ (prompts=['A turtle playing with a ball', 'A monkey playing with a ball'], edit_type=replace, edit_kwargs={'cross_replace_steps': 0.4, 'self_replace_steps': 0.4})\n",
      "Starting next test ðŸŽ¢ (prompts=['A turtle playing with a ball', 'A monkey playing with a ball'], edit_type=replace, edit_kwargs={'cross_replace_steps': 0.4, 'self_replace_steps': 0.4, 'local_blend_words': ['turtle', 'monkey']})\n",
      "Starting next test ðŸŽ¢ (prompts=['A turtle', 'A turtle in a forest'], edit_type=refine, edit_kwargs={'cross_replace_steps': 0.4, 'self_replace_steps': 0.4})\n",
      "Starting next test ðŸŽ¢ (prompts=['A turtle', 'A turtle in a forest'], edit_type=refine, edit_kwargs={'cross_replace_steps': 0.4, 'self_replace_steps': 0.4, 'local_blend_words': ['in', 'a', 'forest']})\n",
      "Starting next test ðŸŽ¢ (prompts=['A smiling turtle', 'A smiling turtle'], edit_type=reweight, edit_kwargs={'cross_replace_steps': 0.4, 'self_replace_steps': 0.4, 'equalizer_words': ['smiling'], 'equalizer_strengths': [5]})\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for t in int_tests: res.append(t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.243 0.233 0.227 0.253 0.242 0.237 0.293 0.292 0.283]\n",
      "[0.243 0.233 0.227 0.253 0.242 0.237 0.293 0.292 0.283]\n",
      "[0.256 0.232 0.209 0.259 0.254 0.229 0.285 0.307 0.295]\n",
      "[0.256 0.232 0.209 0.259 0.254 0.229 0.285 0.307 0.295]\n",
      "[0.006 0.01  0.009 0.003 0.011 0.008 0.014 0.009 0.   ]\n"
     ]
    }
   ],
   "source": [
    "for expected_slice in res: print(expected_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[0.243 0.233 0.227 0.253 0.242 0.237 0.293 0.292 0.283]\n",
    "#[0.243 0.233 0.227 0.253 0.242 0.237 0.293 0.292 0.283]\n",
    "#[0.256 0.232 0.209 0.259 0.254 0.229 0.285 0.307 0.295]\n",
    "#[0.256 0.232 0.209 0.259 0.254 0.229 0.285 0.307 0.295]\n",
    "#[0.006 0.01  0.009 0.003 0.011 0.008 0.014 0.009 0.   ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the test with asserts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "pipe = Prompt2PromptPipeline.from_pretrained(model_id)\n",
    "pipe.to(torch_device)\n",
    "pipe.set_progress_bar_config(disable=None)\n",
    "\n",
    "def test_inference(prompts, edit_type, edit_kwargs, expected_slice, seed=0, n_steps=50):\n",
    "    print(f\"Starting next test ðŸŽ¢ (prompts={prompts}, edit_type={edit_type}, edit_kwargs={edit_kwargs})\")\n",
    "\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    image = pipe(prompts, height=512, width=512, num_inference_steps=n_steps, generator=generator, edit_type=edit_type, edit_kwargs=edit_kwargs, output_type=\"numpy\").images\n",
    "\n",
    "    image_slice = image[0, -3:, -3:, -1]\n",
    "\n",
    "    assert image.shape == (2, 512, 512, 3)\n",
    "    return image_slice.flatten()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks-TPEo7knB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
